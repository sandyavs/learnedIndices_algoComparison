# -*- coding: utf-8 -*-
"""120554-104966 - Sandya Vaigundam Sadasivam - Apr 25, 2023 1135 PM - Big Data - SPN Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1WO9SaOBK5uvEvOq4tJYUdWb3GxdRlIiT
"""

# Main File for Learned Index
import numpy as np
import pandas as pd
import time
from spn.structure.Base import Context
from spn.structure.leaves.parametric.Parametric import Gamma, Uniform, Categorical, Gaussian
from spn.algorithms.LearningWrappers import learn_parametric
from spn.algorithms.Statistics import get_structure_stats
from spn.io.Graphics import plot_spn
from spn.algorithms.Inference import log_likelihood
# from spn.gpu.TensorFlow import optimize_tf
from spn.algorithms.MPE import mpe
from spn.io.Text import spn_to_str_equation
from spn.structure.Base import assign_ids, rebuild_scopes_bottom_up
import psutil
import math
import random  # for self test
from inputFileProcess import *
from bisect import bisect_left  # for binary search over lists

print("Input Dataset: D1/D2/D3 ?")
datasetName = input()
dataframe = pd.read_csv("./dataset_csv/int_10000_20230717_123124_train_aifeynman.ssv", header=None)
column_labels = dataframe.keys()
column_labels = ["x", "y"]
dataframe = dataframe.set_axis(column_labels, axis=1)

# Switch x and y columns, i.e. 0th and 1st column, @author: Avin
col_list = list(dataframe)
col_list[0], col_list[1] = col_list[1], col_list[0]
dataframe.columns = col_list
# print(dataframe.head)

train_data = dataframe.to_numpy()
num_vars = train_data.shape[1]  # Number of variables in the dataset
#TODO: Replace 1st Gaussian->Gamma, 2nd Gaussian->Uniform 
ds_context = Context(parametric_types=[Gaussian, Gaussian]).add_domains(train_data)
# print('ds_context', ds_context)

start_time=time.time()
#Optional learn_parametric param: min_instances_slice=20
spn = learn_parametric(train_data, ds_context)
end_time=time.time()
spn_txt = spn_to_str_equation(spn)
print(spn_txt)
total_build_time = round((end_time-start_time) * 1000, 2) #in ms
print("Build Time:", total_build_time, " ms")

print("Structure Stats:",get_structure_stats(spn))
# print("SPN Structure")
# plot_spn(spn, 'basicspn.png')

column_data = train_data[:, 0].ravel()
column_data_2d = np.column_stack((column_data, np.full(column_data.shape[0], np.nan)))
column_data_2d[:, 0] = column_data_2d[:, 0].astype(float)
pred_list = mpe(spn, column_data_2d)
# print(pred_list)

act_post = train_data[:,1]
pred_post = pred_list[:,1]
pred_result = []
for i in range(0, act_post.shape[0]):
	err_bound = np.absolute(np.subtract(act_post[i], pred_post[i]))
	pred_result.append(err_bound)
pred_result = np.asarray(pred_result)
max_err_bound = pred_result.max()
print('max_err_bound', max_err_bound)

def approx_query(key, spn_model=spn, err_bound=max_err_bound):
	temp_arr_of_key = np.array([key,np.nan])
	arr_of_key = np.array([temp_arr_of_key])
	print('arr_of_key', arr_of_key, type(arr_of_key), arr_of_key.shape)
	res = mpe(spn_model, arr_of_key)
	begin = res[0][1] - err_bound
	end = res[0][1] + err_bound
	if(begin < 0):
		begin = 0
	# elif(end < 0):
	# 	end = 0
	print({ "begin": begin, "end": end })
	return { "begin": begin, "end": end }

def find_ge_index(sorted_list, x):
    """
    Assuming a sorted list and a given value 'x'
    find the index of the element greater than or equal to x

    Parameters
    ----------
    sorted_list : list

    some_val : query index

    Returns
    -------
    None.

    """

    i = bisect_left(sorted_list, x)
    if i != len(sorted_list):
        return i

    return ValueError

# result is a range of start index and end index estimation
# for the query point, may be in floating point
dict_result = {
	"exist": {
		"query": [],
		"binary_search": [],
		"mean": 0,
		"std": 0
	},
	"non-exist": {
		"query": [],
		"binary_search": [],
		"mean": 0,
		"std": 0
	},
	"approx_accuracy": 0,
	"bsearch_accuracy": 0
}
def calc_runtime(random_query_point_range, key):
	for query_point in random_query_point_range:
		#approx_query runtime
		query_search_bound_stime = time.time()
		query_result = approx_query(query_point)
		print("query result: ", query_result)
		query_search_bound_etime = time.time()
		query_search_bound_time = (query_search_bound_etime - query_search_bound_stime) * 1000
		print("approximate query - runtime", round(query_search_bound_time, 2), " ms")

		#binary search runtime
		query_last_mile_stime = time.time()
		start_slice_index = math.ceil(query_result['begin'])
		end_slice_index = math.ceil(query_result['end'])
		# print('start_slice_index', start_slice_index, end_slice_index)
		binary_search_result = find_ge_index(column_data_as_list[start_slice_index:end_slice_index], query_point)
		print("binary search result: ", binary_search_result)
		query_last_mile_etime = time.time()
		query_last_mile_time = (query_last_mile_etime - query_last_mile_stime) * 1000
		print("binary search - runtime", round(query_last_mile_time, 2), " ms")

		dict_result[key]["query"].append(query_search_bound_time)
		dict_result[key]["binary_search"].append(query_last_mile_time)
	return dict_result

def round_decimal(args, roundOff = 2):
	return round(args, roundOff)

def calc_mean_std(dict, key):
	approx_query_runtime_list = dict[key]['query']
	bsearch_runtime_list = dict[key]['binary_search']

	mean_for_approx_runtime = round_decimal(np.mean(approx_query_runtime_list))
	std_for_approx_runtime = round_decimal(np.std(approx_query_runtime_list))
	print(key, "- approx_list", ' mean - ', mean_for_approx_runtime, " ms", 'std - ', std_for_approx_runtime, " ms",)
	dict_result[key]['mean'] = mean_for_approx_runtime
	dict_result[key]['std'] = std_for_approx_runtime

	mean_for_bsearch_runtime = round_decimal(np.mean(bsearch_runtime_list))
	std_for_bsearch_runtime = round_decimal(np.mean(bsearch_runtime_list))
	print(key, '- binary_list', ' mean - ', mean_for_bsearch_runtime, " ms", ' std - ', std_for_bsearch_runtime, " ms",)
	return

print('Prediction for existing keys: ')
column_data_as_list = column_data.tolist()
random_query_point_range = random.sample(column_data_as_list,  100)
dict_result = calc_runtime(random_query_point_range, 'exist')
calc_mean_std(dict_result, 'exist')

print("\n Prediction for non-existing keys: ")
random_query_point_range = random.sample(list(set(range(min(column_data)+1, max(column_data)))-set(column_data)), 100)
dict_result = calc_runtime(random_query_point_range, 'non-exist')
calc_mean_std(dict_result, 'non-exist')

dict_result["approx_accuracy"] = round_decimal(dict_result["exist"]["mean"] - dict_result["non-exist"]["mean"])
print("approx_accuracy", dict_result["approx_accuracy"])
# # Get the current process object
# process = psutil.Process()
# # Get the memory usage in bytes
# mem_info = process.memory_info().rss
# # Convert to megabytes
# mem_usage = mem_info / 1024 / 1024
# print(f"Memory usage: {mem_usage:.2f} MB")

# jsonObj = read_json_file()
# jsonObj = modify_json_data(read_json_file(), 'SPN', datasetName, total_build_time, total_query_time)
# write_json_file(jsonObj)

# column_data_as_list = train_data
# column_data_as_list = dataframe[['y']].to_numpy().ravel()
# column_data_as_list = column_data_as_list.tolist()
# noOfQueries = 1
# print('column_data_as_list', column_data_as_list[:10])
# random_query_point_range = random.sample(column_data_as_list,  noOfQueries)
# print('queryPoint', random_query_point_range)

# query_point = np.asarray(random_query_point_range)
# print('queryPoint', query_point, len(query_point))
# ll = log_likelihood(spn, query_point)
# print('log likelihood', ll, np.exp(ll))

# optimized_spn = optimize_tf(spn, query_point)
# lloptimized = log_likelihood(optimized_spn, query_point)
# print('lloptimized', lloptimized, np.exp(lloptimized))